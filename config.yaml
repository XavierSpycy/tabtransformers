# task-specific configuration
# replace it with your own data
target_column: your_target_column_name
# replace them all with your own data
categorical_features:
  - feature_1
  - feature_2
  # ...
# replace them all with your own data
continuous_features: 
  - feature_3
  - feature_4
  # ...
# disregard if you don't have an index column
index_col: null
# replace them all with your own data or set to null if you don't have any
target_mapping: 
  class_1: 0
  class_2: 1
  class_3: 2
  # ...

# random seed
seed: 0

# data - only supports csv files at the moment
data_dir: data
train_file: train.csv
test_file: test.csv
# refer to: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split
train_test_split: 
  test_size: 0.15
  shuffle: True
  # ... add some other parameters if needed expect for random_state

# model and related hypterparameters
model_class: tab_transformer # or feature_tokenizer_transformer
model_kwargs:
  output_dim: 3 # number of classes for classification tasks; 1 for regression tasks
  embedding_dim: 64
  nhead: 8
  num_encoder_layers: 3
  dim_feedforward: 128
  mlp_hidden_dim: 
    - 32
    # can continue to add more hidden layers in the format of e.g. "- 64"; must be a list
  # 'relu' or 'gelu'
  # check: https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer for more details
  activation: 'relu'
  attn_dropout_rate: 0.1
  ffn_dropout_rate: 0.1

# loss function
# pass the class name of the loss function in PyTorch in lowercase; split by '_'
loss_function: cross_entropy
# pass the parameters of the loss function in a dictionary format; refer to PyTorch documentation
loss_kwargs: null

# optimizer
optim: adamw # pass the class name of the optimizer in PyTorch in lowercase
optim_kwargs:
  learning_rate: 1.0e-3
  weight_decay: 0.0
  # Can add more parameters if needed refer to PyTorch documentation

# learning rate scheduler
# supports almost all the learning rate schedulers in PyTorch 
# except for LambdaScheduler, MultiplicativeLR, ChainedScheduler and SequentialLR
lr_scheduler: reduce_on_plateau
lr_scheduler_kwargs: 
  mode: max
  factor: 0.1
  patience: 10
# set to true if you want to use a custom metric for the learning rate scheduler
lr_scheduler_by_custom_metric: true

# training hyperparameters
train_batch_size: 64
eval_batch_size: 64
epochs: 100

# early stopping
early_stopping: true
early_stopping_patience: 5
early_stopping_start_from: 20

# evaluation metric
# temporarily supports only f1_score_macro and root_mean_squared_logarithmic_error
custom_metric: null
# set to false if the lower value is better
is_greater_better: true

# output
to_submssion: true
submission_file: submission.csv